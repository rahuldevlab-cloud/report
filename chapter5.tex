% ============================================================
% CHAPTER 6: RESULTS (Extended Version with Makespan and Flowtime)
% ============================================================
\chapter{Results}
\label{chap:results}

\section{Introduction}
This chapter presents the experimental evaluation of the proposed Adaptive Diffusion Convolution (ADC) enhanced Multi-Robot Path Planning (MRPP) framework. First, the experimental setup is described, including implementation specifics, baseline models for comparison, evaluation metrics, and the training configuration. Subsequently, a comprehensive analysis of the results is presented, comparing the performance of the ADC-based approach with traditional fixed-K Graph Neural Network (GNN) baselines across various scenarios. The evaluation focuses on success rates, average makespan, flowtime, adaptability to different environmental conditions (specifically varying obstacle densities), generalization capabilities, and computational efficiency. Ablation studies are also conducted to understand the impact of key components of the ADC mechanism.

\section{Experimental Setup}
\label{sec:exp_setup}

All experiments were conducted on synthetic grid-world environments as described in Chapter \ref{chap:dataset}. The following subsections detail the setup.

\subsection{Implementation Details}
\label{subsec:implementation_details}
The proposed ADC-MRPP framework and all baseline models were implemented using the PyTorch deep learning library \cite{Paszke2019PyTorch}.
\begin{itemize}
    \item \textbf{Software Environment:} Python 3.11.10, PyTorch 2.6.0, CUDA 12.4
    \item \textbf{Hardware Environment:} Experiments were run on a machine equipped with dual Intel Xeon Gold 6326 CPUs (32 cores, 64 threads) running at 2.90 GHz, 250 GB RAM, and an NVIDIA L4 GPU with 23 GB VRAM.
    \item \textbf{CNN Encoder Architecture:} A shared CNN encoder processed the local $5 \times 5 \times 3$ FOV input for each agent. It consisted of 3 convolutional layers (32 filters, kernel size 3x3, stride 1, padding 1; 64 filters, kernel size 3x3, stride 1, padding 1; 64 filters, kernel size 3x3, stride 1, padding 1), each followed by ReLU activation and Max Pooling (kernel size 2x2, stride 2 for the first two, no pooling for the last). The flattened output was passed through a fully connected layer to produce a 128-dimensional feature embedding.
    \item \textbf{GNN Layer Architecture:}
        \begin{itemize}
            \item For fixed-K GNN baselines, Graph Convolutional Network (GCN) layers \cite{Kipf2017GCN} were used with a hidden dimension of 128. The number of hops $K$ was varied: $K \in \{1, 2, 3, 4\}$.
            \item For the ADC-MRPP model, the ADC layer (hidden dimension 128) replaced the GCN layer. The Taylor expansion truncation order $K_{trunc}$ for ADC was set to 10, unless specified otherwise.
        \end{itemize}
        Both GNN types used ReLU activation.
    \item \textbf{MLP Action Selector:} A 2-layer MLP (hidden layer: 128 units, ReLU; output layer: 5 units for actions) processed the GNN output.
\end{itemize}

\subsection{Baselines for Comparison}
\label{subsec:baselines}
The ADC-MRPP model was compared against:
\begin{itemize}
    \item \textbf{Fixed-K GNN (K=1, K=2, K=3, K=4):} Standard GCN-based communication with varying fixed hop ranges.
    \item \textbf{ADC Ablations:} ADC with fixed diffusion time ($t$) and ADC with minimal Taylor truncation ($K_{trunc}=1$).
\end{itemize}
All models shared the same CNN encoder and MLP action selector.

\subsection{Evaluation Metrics}
\label{subsec:evaluation_metrics}
Models were evaluated on unseen test scenarios using the following standard metrics:
\begin{itemize}
    \item \textbf{Success Rate (SR):} The percentage of test episodes where all $N$ robots successfully reached their respective goal locations without any collisions (robot-robot or robot-obstacle) within a predefined maximum number of time steps (120 steps).
    \item \textbf{Average Makespan (AM):} The average number of time steps taken for the last robot to reach its goal in successful episodes.
    \item \textbf{Flowtime (FT):} The sum of the number of time steps each individual robot took to reach its goal, averaged over successful episodes.
    \item \textbf{Average Inference Time:} Average time in milliseconds (ms) per agent per step for the model to compute an action.
    \item \textbf{Number of Parameters:} Total trainable parameters in the model.
\end{itemize}
Results are averaged over multiple test runs/seeds if applicable from the source data. For unsuccessful episodes, robots that do not reach their goal are typically excluded from AM and FT calculations.

\subsection{Training Configuration}
\label{subsec:training_config}
All models were trained using imitation learning on the expert dataset generated by Conflict-Based Search (CBS), as detailed in Chapter \ref{chap:dataset}.
\begin{itemize}
    \item \textbf{Expert Planner:} CBS \cite{Sharon2015CBS}.
    \item \textbf{Loss Function:} Cross-Entropy loss.
    \item \textbf{Optimizer:} Adam optimizer \cite{Kingma2014Adam}.
    \item \textbf{Learning Rate:} $1 \times 10^{-4}$.
    \item \textbf{Batch Size:} 32.
    \item \textbf{Number of Epochs:} 100, with early stopping (patience 10 epochs on validation SR).
    \item \textbf{Environment Parameters (Shared for all experiments unless specified):}
        \begin{itemize}
            \item Map Size: 10x10.
            \item Number of Robots ($N$): 5.
            \item Communication Radius ($r_{comm}$): 6 cells.
            \item Field-of-View (FOV): $5 \times 5$ (pad=3).
        \end{itemize}
    \item \textbf{Dataset Splits and Sizes:}
    \begin{itemize}
        \item \textbf{Training Sets:} Models were trained separately on datasets with 10\%, 20\%, and 30\% obstacle densities. The number of unique training scenarios (cases) for each density were:
            \begin{itemize}
                \item 10\% obstacle density (o10): 4767 cases.
                \item 20\% obstacle density (o20): 4255 cases.
                \item 30\% obstacle density (o30): 2346 cases.
            \end{itemize}

        \item \textbf{Test Sets:} Dedicated test sets were used to evaluate the performance of the trained models. The number of test cases for each density were:
            \begin{itemize}
                \item 10\% obstacle density (o10): 966 cases.
                \item 20\% obstacle density (o20): 859 cases.
                \item 30\% obstacle density (o30): 482 cases.
            \end{itemize}

        \item \textbf{Validation Sets:} Validation sets were utilized during the training process, for instance, for hyperparameter tuning or early stopping. The number of validation cases for each density were:
            \begin{itemize}
                \item 10\% obstacle density (o10): 949 cases.
                \item 20\% obstacle density (o20): 853 cases.
                \item 30\% obstacle density (o30): 471 cases.
            \end{itemize}
    \end{itemize}
    \newpage % Consider if this page break is necessary here
\end{itemize}

\begin{comment}
\section{Overall Performance Analysis (Trained on 10\% Obstacles)}
\label{sec:overall_performance_10D}
This section presents the performance of models trained on \textbf{TrainSet-10D-5R-10M} (10x10 map, 5 robots, 10\% obstacles), averaged across test sets with 10\%, 20\%, and 30\% obstacle densities.

\begin{table}[htbp]
    \centering
    \caption{Overall Performance (10x10 Maps, 5 Robots, Avg. over 10-30\% Test Obstacle Densities). Models trained on 10\% Obstacles.}
    \label{tab:overall_perf_10D}
    \scriptsize % For smaller font
    \begin{tabular}{lccccc}
        \toprule
        Model & SR \% & AM & FT & Avg. Inf. Time (ms) & Params \\
        \midrule
        GCN (K=1) & 54.59 & 10.32 & 100.21 & 0.79 & 39717 \\ % Averaging (0.7453+0.5087+0.3838)/3 etc.
        GCN (K=2) & 58.31 & 10.52 & 96.34 & 0.88 & 47909 \\
        GCN (K=3) & 58.21 & 10.41 & 97.99 & 0.90 & 56101 \\
        GCN (K=4) & 59.92 & 10.51 & 95.66 & 0.95 & 64293 \\
        \midrule
        \textbf{ADC-Main} & \textbf{56.92} & \textbf{10.51} & \textbf{96.71} & \textbf{1.37} & \textbf{39718} \\
        ADC-FixedT & 58.65 & 10.78 & 95.83 & 1.40 & 39717 \\
        ADC-K1 & 55.36 & 10.35 & 99.53 & 1.05 & 39718 \\
        \bottomrule
    \end{tabular}
\end{table}
As shown in Table \ref{tab:overall_perf_10D}, when trained on 10\% obstacle density, the GCN (K=4) model achieved the highest average SR. The ADC-Main model demonstrated a competitive SR and path efficiency metrics. Inference times for ADC models are higher due to the more complex aggregation.
%*(Note: The provided "concise paper" Table I had different values; the values %here are derived by averaging the `_o10_p5` trained models across the three %test densities from `b.txt`. Adjust interpretation if your Table I was from a %different data source/aggregation.)*
\end{comment}

\section{Performance Across Varying Obstacle Densities}
\label{sec:perf_obstacle_density_detailed}

\subsection{Models Trained on 10\% Obstacle Density (TrainSet-10D-5R-10M)}
\label{subsec:perf_10D_train_detailed}
Evaluation on 10x10 maps with 5 robots and varying test obstacle densities.

\begin{table}[htbp]
    \centering
    \caption{Performance on 10x10 Maps (5 Robots) with Varying Test Obstacle Densities. Models trained on 10\% Obstacles.}
    \label{tab:density_perf_10D_train}
    \scriptsize % For smaller font
    \begin{tabular}{llccccc}
        \toprule
        Test Density & Model & SR \% & AM & FT & Avg. Inf. Time (ms) & Params \\
        \midrule
        \multirow{7}{*}{10\%}
        & GCN (K=1) & 0.7453 & 10.2972 & 65.5435 & 0.9750 & 39717 \\
        & GCN (K=2) & 0.8095 & 10.4616 & 57.4741 & 0.9906 & 47909 \\
        & GCN (K=3) & 0.7909 & 10.4084 & 59.9834 & 1.0092 & 56101 \\
        & GCN (K=4) & 0.8043 & 10.5122 & 57.8810 & 1.0339 & 64293 \\
        & ADC-Main & 0.7547 & 10.3443 & 64.0611 & 1.4841 & 39718 \\
        & \textbf{ADC-FixedT} & \textbf{0.8116} & 10.5472 & \textbf{57.6398} & 1.4781 & 39717 \\
        & ADC-K1 & 0.7650 & 10.3775 & 63.1387 & 0.9733 & 39718 \\
        \midrule
        \multirow{7}{*}{20\%}
        & GCN (K=1) & 0.5087 & 10.3936 & 102.9569 & 0.6594 & 39717 \\
        & GCN (K=2) & 0.5704 & 10.7347 & 95.0419 & 0.6795 & 47909 \\
        & GCN (K=3) & \textbf{0.5821} & 10.7720 & \textbf{93.7835} & 0.6974 & 56101 \\
        & GCN (K=4) & 0.5634 & 10.6921 & 95.7637 & 0.7143 & 64293 \\
        & ADC-Main & 0.5588 & 10.6854 & 96.0687 & 1.1419 & 39718 \\
        & ADC-FixedT & 0.5553 & 10.9518 & 97.8778 & 1.2411 & 39717 \\
        & ADC-K1 & 0.5390 & 10.5032 & 99.1886 & 1.0532 & 39718 \\
        \midrule
        \multirow{7}{*}{30\%}
        & GCN (K=1) & 0.3838 & 10.2595 & 132.1328 & 0.7271 & 39717 \\
        & GCN (K=2) & 0.3693 & 10.3539 & 134.1203 & 0.9786 & 47909 \\
        & GCN (K=3) & 0.3734 & 10.1833 & 133.9938 & 0.9951 & 56101 \\
        & GCN (K=4) & \textbf{0.3942} & 10.4579 & 131.3320 & 1.0168 & 64293 \\
        & ADC-Main & \textbf{0.3942} & 10.5947 & \textbf{130.0083} & 1.4707 & 39718 \\
        & ADC-FixedT & 0.3921 & 10.8677 & 131.9917 & 1.4689 & 39717 \\
        & ADC-K1 & 0.3568 & 10.1686 & 136.2614 & 1.1294 & 39718 \\
        \bottomrule
    \end{tabular}
\end{table}
Table \ref{tab:density_perf_10D_train} shows that when trained on 10\% obstacles, ADC-FixedT performs best on 10\% test density. GCN (K=3) performs best on 20\% test density. In the test density 30\%, GCN (K = 4) and ADC-Main show the highest SR, with ADC-Main having slightly better flowtime. Even though in the obstacle case 20\% GCN (K = 3) performs better, the ADC results were also comparable to them.
\newpage

\subsection{Models Trained on 20\% Obstacle Density (TrainSet-20D-5R-10M)}
\label{subsec:perf_20D_train_detailed}
Evaluation on 10x10 maps with 5 robots and varying test obstacle densities.

\begin{table}[htbp]
    \centering
    \caption{Performance on 10x10 Maps (5 Robots) with Varying Test Obstacle Densities. Models trained on 20\% Obstacles.}
    \label{tab:density_perf_20D_train}
    \scriptsize % For smaller font
    \begin{tabular}{llccccc}
        \toprule
        Test Density & Model & SR \% & AM & FT & Avg. Inf. Time (ms) & Params \\
        \midrule
        \multirow{7}{*}{10\%}
        & GCN (K=1) & 0.7422 & 10.3124 & 65.9400 & 0.9029 & 39717 \\
        & GCN (K=2) & 0.7899 & 10.4325 & 60.6387 & 0.9870 & 47909 \\
        & GCN (K=3) & 0.7774 & 10.5047 & 60.9731 & 1.0050 & 56101 \\
        & GCN (K=4) & 0.8085 & 10.4571 & 57.7153 & 1.0295 & 64293 \\
        & ADC-Main & 0.7712 & 10.2497 & 62.9959 & 1.4694 & 39718 \\
        & ADC-FixedT & \textbf{0.8302} & 10.5673 & \textbf{55.7660} & 1.4701 & 39717 \\
        & ADC-K1 & 0.7588 & 10.2906 & 64.0890 & 1.1215 & 39718 \\
        \midrule
        \multirow{7}{*}{20\%}
        & GCN (K=1) & 0.5553 & 10.6059 & 96.7078 & 0.8863 & 39717 \\
        & GCN (K=2) & 0.6217 & 10.9438 & 87.4098 & 0.9858 & 47909 \\
        & GCN (K=3) & 0.6100 & 10.9351 & 89.3190 & 0.9985 & 56101 \\
        & GCN (K=4) & 0.6193 & 10.8741 & 88.2037 & 1.0058 & 64293 \\
        & ADC-Main & 0.5856 & 10.6998 & 92.4307 & 1.4283 & 39718 \\
        & ADC-FixedT & \textbf{0.6461} & 11.3063 & \textbf{85.3679} & 1.4252 & 39717 \\ % ADC-FixedT has higher SR here
        & ADC-K1 & 0.5925 & 10.6503 & 91.2980 & 1.0954 & 39718 \\
        \midrule
        \multirow{7}{*}{30\%}
        & GCN (K=1) & 0.4440 & 10.7009 & 123.3195 & 0.9413 & 39717 \\
        & GCN (K=2) & 0.4668 & 11.0267 & 121.5207 & 0.9631 & 47909 \\
        & GCN (K=3) & 0.4419 & 10.6432 & 124.0249 & 0.6841 & 56101 \\
        & GCN (K=4) & \textbf{0.4689} & 10.7124 & \textbf{120.6639} & 0.6937 & 64293 \\
        & ADC-Main & 0.4398 & 10.7972 & 123.6888 & 1.1152 & 39718 \\
        & ADC-FixedT & 0.4627 & 11.1300 & 121.4315 & 1.1141 & 39717 \\
        & ADC-K1 & 0.4564 & 10.7364 & 121.3299 & 0.7984 & 39718 \\
        \bottomrule
    \end{tabular}
\end{table}
When trained on 20\% obstacles (Table \ref{tab:density_perf_20D_train}), ADC-FixedT shows strong performance on the 10\% test set. GCN (K=2) and ADC-FixedT are competitive on the 20\% test set, while GCN (K=4) leads on the 30\% test set.
\newpage

\subsection{Models Trained on 30\% Obstacle Density (TrainSet-30D-5R-10M)}
\label{subsec:perf_30D_train_detailed}
Evaluation on 10x10 maps with 5 robots and varying test obstacle densities.

\begin{table}[htbp]
    \centering
    \caption{Performance on 10x10 Maps (5 Robots) with Varying Test Obstacle Densities. Models trained on 30\% Obstacles.}
    \label{tab:density_perf_30D_train}
    \scriptsize % For smaller font
    \begin{tabular}{llccccc}
        \toprule
        Test Density & Model & SR \% & AM & FT & Avg. Inf. Time (ms) & Params \\
        \midrule
        \multirow{7}{*}{10\%}
        & GCN (K=1) & 0.7640 & 10.4024 & 62.8364 & 0.6742 & 39717 \\
        & GCN (K=2) & 0.7702 & 10.4368 & 62.4255 & 0.6899 & 47909 \\
        & GCN (K=3) & 0.7712 & 10.4940 & 62.9431 & 0.7104 & 56101 \\
        & GCN (K=4) & 0.7743 & 10.5508 & 61.5280 & 0.7270 & 64293 \\
        & ADC-Main & 0.7588 & 10.3915 & 63.5248 & 1.1517 & 39718 \\
        & ADC-FixedT & \textbf{0.8147} & 10.6861 & \textbf{56.7008} & 1.1504 & 39717 \\
        & ADC-K1 & 0.7629 & 10.4152 & 63.1925 & 0.8344 & 39718 \\
        \midrule
        \multirow{7}{*}{20\%}
        & GCN (K=1) & 0.5844 & 10.6574 & 93.3946 & 0.6666 & 39717 \\
        & GCN (K=2) & 0.6135 & 10.8121 & 89.6217 & 0.6868 & 47909 \\
        & GCN (K=3) & 0.6042 & 11.0039 & 90.7392 & 0.7057 & 56101 \\
        & GCN (K=4) & 0.5925 & 10.7603 & 91.7474 & 0.7233 & 64293 \\
        & ADC-Main & 0.5902 & 10.7239 & 92.1409 & 1.1480 & 39718 \\
        & ADC-FixedT & \textbf{0.6531} & 11.2531 & \textbf{84.8929} & 1.1463 & 39717 \\ % ADC-FixedT has higher SR here
        & ADC-K1 & 0.5541 & 10.6176 & 97.0303 & 0.8273 & 39718 \\
        \midrule
        \multirow{7}{*}{30\%}
        & GCN (K=1) & 0.4336 & 10.7273 & 125.0000 & 0.6658 & 39717 \\
        & GCN (K=2) & 0.4419 & 10.8498 & 123.2988 & 0.6856 & 47909 \\
        & GCN (K=3) & 0.4606 & 11.2387 & 120.5456 & 0.7035 & 56101 \\
        & GCN (K=4) & 0.4710 & 10.5903 & 119.8278 & 0.7218 & 64293 \\
        & ADC-Main & 0.4585 & 10.7059 & 121.3755 & 1.1448 & 39718 \\
        & ADC-FixedT & \textbf{0.4751} & 11.3930 & \textbf{119.5456} & 1.1450 & 39717 \\
        & ADC-K1 & 0.4502 & 10.7419 & 122.9979 & 0.8250 & 39718 \\
        \bottomrule
    \end{tabular}
\end{table}
When trained on 30\% obstacles (Table \ref{tab:density_perf_30D_train}), ADC-FixedT demonstrates the best performance across all test densities, particularly excelling on the 10\% test set. GCN models with K=2 or K=4 are competitive on the higher density test sets.
\newpage

\begin{comment}
\section{Ablation Studies}
\label{sec:ablation_studies_detailed}
Conducted on the `map10x10\_r5\_o10\_p5\_test` set, with models trained on **TrainSet-10D-5R-10M**.

\begin{table}[htbp]
    \centering
    \caption{Ablation Study Results (Test: 10x10 Map, 5 Robots, 10\% Obstacles). Models trained on 10\% Obstacles.}
    \label{tab:ablation}
    \scriptsize
    \begin{tabular}{lccccc}
        \toprule
        Model Variant (Trained on 10\% Obst.) & SR \% & AM & FT & Avg. Inf. Time (ms) & Params \\
        \midrule
        \textbf{ADC-Main ($K_{trunc}$=10)} & 0.7547 & 10.3443 & 64.0611 & 1.4841 & 39718 \\
        ADC-FixedT ($K_{trunc}$=10) & \textbf{0.8116} & 10.5472 & \textbf{57.6398} & 1.4781 & 39717 \\
        ADC-K1 ($K_{trunc}$=1) & 0.7650 & 10.3775 & 63.1387 & 0.9733 & 39718 \\
        \midrule
        GCN (K=1) & 0.7453 & 10.2972 & 65.5435 & 0.9750 & 39717 \\
        GCN (K=2) & 0.8095 & 10.4616 & 57.4741 & 0.9906 & 47909 \\
        GCN (K=3) & 0.7909 & 10.4084 & 59.9834 & 1.0092 & 56101 \\
        GCN (K=4, Best Fixed-K) & 0.8043 & 10.5122 & 57.8810 & 1.0339 & 64293 \\
        \bottomrule
    \end{tabular}
\end{table}
The ablation studies in Table \ref{tab:ablation} (tested on 10\% obstacle density) show that ADC-FixedT surprisingly outperforms ADC-Main on this specific test set. ADC-K1 also shows reasonable performance, close to GCN (K=1) but with more parameters. This suggests that for this specific train/test combination, a well-chosen fixed $t$ (as in ADC-FixedT) can be very effective.
\end{comment}

\begin{comment}
\section{Generalization Capability}
\label{sec:generalization_detailed}

\subsection{Generalization from Low (10\%) to Higher Obstacle Densities}
Models trained on 10\% obstacles (TrainSet-10D-5R-10M) tested on 20\% and 30% densities.

\begin{table}[htbp]
    \centering
    \caption{Generalization from 10\% Training Obstacles to Higher Test Densities (10x10 Map, 5 Robots)}
    \label{tab:gen_10D_to_high}
    \scriptsize
    \begin{tabular}{llccccc}
        \toprule
        Tested On & Model (Trained on 10\% Obst.) & SR \% & AM & FT & Avg. Inf. Time (ms) & Params \\
        \midrule
        \multirow{2}{*}{20\% Obstacles}
        & GCN (K=3, Best SR) & \textbf{0.5821} & 10.7720 & \textbf{93.7835} & 0.6974 & 56101 \\
        & ADC-Main & 0.5588 & 10.6854 & 96.0687 & 1.1419 & 39718 \\
        \midrule
        \multirow{2}{*}{30\% Obstacles}
        & GCN (K=4, Best SR) & \textbf{0.3942} & 10.4579 & 131.3320 & 1.0168 & 64293 \\
        & ADC-Main & \textbf{0.3942} & 10.5947 & \textbf{130.0083} & 1.4707 & 39718 \\
        \bottomrule
    \end{tabular}
\end{table}
When generalizing from 10\% training density to higher test densities (Table \ref{tab:gen_10D_to_high}), the best performing GCN model (K=3 for 20\% test, K=4 for 30\% test) shows a slight SR advantage or matches ADC-Main. ADC-Main shows competitive flowtime in the 30% case.


\subsection{Generalization from High (30\%) to Lower Obstacle Densities}
Models trained on 30\% obstacles (TrainSet-30D-5R-10M) tested on 10% and 20% densities.

\begin{table}[htbp]
    \centering
    \caption{Generalization from 30\% Training Obstacles to Lower Test Densities (10x10 Map, 5 Robots)}
    \label{tab:gen_30D_to_low}
    \scriptsize
    \begin{tabular}{llccccc}
        \toprule
        Tested On & Model (Trained on 30\% Obst.) & SR \% & AM & FT & Avg. Inf. Time (ms) & Params \\
        \midrule
        \multirow{2}{*}{10\% Obstacles}
        & GCN (K=4, Best SR) & 0.7743 & 10.5508 & 61.5280 & 0.7270 & 64293 \\
        & ADC-FixedT & \textbf{0.8147} & 10.6861 & \textbf{56.7008} & 1.1504 & 39717 \\
        \midrule
        \multirow{2}{*}{20\% Obstacles}
        & GCN (K=2, Best SR) & 0.6135 & 10.8121 & 89.6217 & 0.6868 & 47909 \\
        & ADC-FixedT & \textbf{0.6531} & 11.2531 & \textbf{84.8929} & 1.1463 & 39717 \\
        \bottomrule
    \end{tabular}
\end{table}
When generalizing from 30\% training density to lower test densities (Table \ref{tab:gen_30D_to_low}), ADC-FixedT significantly outperforms the GCN baselines in SR and FT. This suggests that learning or setting an adaptive diffusion range can be very beneficial when the training distribution (high clutter) differs significantly from the test distribution (lower clutter).
\end{comment}

\section{Qualitative Analysis and Case Studies}
\label{sec:qualitative_analysis_detailed}
*(This section remains for your figures and qualitative descriptions. Plan for figures here, e.g., \reffig{fig:case_study1}, \reffig{fig:case_study2}).*
Case studies will visualize trajectories in challenging scenarios (e.g., high clutter, narrow passages) comparing ADC-MRPP against the best fixed-K GNN. The focus will be on illustrating how ADC's adaptive communication facilitates deadlock resolution or more efficient coordination where fixed-K methods struggle.

\begin{comment}
\subsection{Analysis of Learned Diffusion Parameter $t$}
\label{subsec:learned_t_detailed}
*(This section remains for your analysis of learned 't' values. The `b.txt` file does not contain learned 't' values.)*
The average learned $t$ for the ADC-Main model (trained on TrainSet-10D-5R-10M) was approximately [Value based on your training logs]. This suggests a preference for a communication range that effectively incorporates information from [X-Y] hop neighbors with diminishing influence. *(Expand with histograms or plots of learned t values if available, or if t is learned per layer/channel.)*
\end{comment}

\section{Computational Performance}
\label{sec:comp_perf_detailed}

\subsection{Inference Time}
\label{subsec:inference_time_detailed}
Average inference time per agent per step based on models trained on 10\% obstacles, tested on the 10\% obstacle density test set (`map10x10_r5_o10_p5_test`).

\begin{table}[htbp]
    \centering
    \caption{Average Inference Time per Agent per Step (ms). Models trained on 10\% Obstacles.}
    \label{tab:inference_time_detailed}
    \begin{tabular}{lc}
        \toprule
        Model & Avg. Inference Time (ms) \\
        \midrule
        GCN (K=1) & 0.9750 \\
        GCN (K=2) & 0.9906 \\
        GCN (K=3) & 1.0092 \\
        GCN (K=4) & 1.0339 \\
        \midrule
        ADC-Main ($K_{trunc}$=10) & 1.4841 \\
        ADC-FixedT ($K_{trunc}$=10) & 1.4781 \\
        ADC-K1 ($K_{trunc}$=1) & 0.9733 \\
        \bottomrule
    \end{tabular}
\end{table}
Inference times for ADC-Main and ADC-FixedT are higher than GCN models due to the Taylor expansion (Table \ref{tab:inference_time_detailed}). ADC-K1, with minimal expansion, is comparable to GCN (K=1). All models remain suitable for many real-time applications.

\begin{comment}
\subsection{Training Time}
\label{subsec:training_time_detailed}
*(Training time is not in b.txt. This table needs manual data or to be described qualitatively.)*
\begin{table}[htbp]
    \centering
    \caption{Approximate Total Training Time (hours) for 100 Epochs}
    \label{tab:training_time_detailed}
    \begin{tabular}{lc}
        \toprule
        Method & Training Time (hours) \\
        \midrule
        Fixed-K GNN (Avg. K=1 to 4) & ~[Value] \\
        \midrule
        ADC-MRPP ($K_{trunc}$=10) & ~[Value] \\
        \bottomrule
    \end{tabular}
\end{table}
ADC introduces a [modest/significant] increase in training time (Table \ref{tab:training_time_detailed}) due to the computation of diffusion coefficients and potentially a more complex loss landscape for the parameter $t$.
\end{comment}

\section{Chapter Summary}
\label{sec:results_summary_detailed}
The experimental results indicate that the performance benefits of ADC are nuanced and depend on the specific training and testing conditions. While the fully adaptive ADC-Main model shows competitive performance, particularly in generalization to highly cluttered unseen environments when trained on sparse data, versions like ADC-FixedT can achieve very strong results if the fixed diffusion time is well-suited to the data distribution (e.g., outperforming ADC-Main on 10\% test density when trained on 10\% or 30\% density). The fixed-K GCN models also remain strong contenders, with the optimal K varying by scenario. The adaptive nature of ADC (especially ADC-FixedT when generalizing from high to low density) demonstrates a clear potential for improved robustness when train and test distributions differ significantly. However, the learnable $t$ in ADC-Main did not consistently outperform a well-tuned fixed $t$ or the best fixed-K GCN in all scenarios presented. The computational overhead for ADC models during inference is noticeable but generally acceptable. These findings suggest that while adaptive diffusion is a promising direction, further investigation into the learning dynamics of the diffusion parameter $t$ and the choice of $K_{trunc}$ is warranted to consistently harness its full potential across all conditions.